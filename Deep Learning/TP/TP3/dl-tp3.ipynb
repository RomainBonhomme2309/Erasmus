{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "abb49b5c-7533-4bf0-bcce-37373d6fa072",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d1502bbeee981ecc3a7a4464df92be0c",
     "grade": false,
     "grade_id": "cell-a975d7bfb06f30ae",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# IS319 - Deep Learning\n",
    "\n",
    "## TP3 - Recurrent neural networks\n",
    "\n",
    "Credits: Andrej Karpathy\n",
    "\n",
    "The goal of this TP is to experiment with recurrent neural networks for a character-level language model to generate text that looks like training text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "16d03ccd-089a-4553-bd96-7bd0c0f71949",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4c736b5-43df-4aca-9ba3-dd34ed8366f4",
   "metadata": {},
   "source": [
    "## 1. Text data preprocessing\n",
    "\n",
    "Several text datasets are provided, feel free to experiment with different ones throughout the TP. At the beginning, use a small subset of a given dataset (for example use only 10k characters)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e56a81b9-b733-4d87-b8d7-2184d5e5c2de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset `baudelaire.txt` contains 10000 characters.\n",
      "Excerpt of the dataset:\n",
      "LES FLEURS DU MAL\n",
      "\n",
      "par\n",
      "\n",
      "CHARLES BAUDELAIRE\n",
      "\n",
      "\n",
      "AU LECTEUR\n",
      "\n",
      "\n",
      "La sottise, l'erreur, le péché, la lésine,\n",
      "Occupent nos esprits et travaillent nos corps,\n",
      "Et nous alimentons nos aimables remords,\n",
      "Comme les mendiants nourrissent leur vermine.\n",
      "\n",
      "Nos péchés sont têtus, nos repentirs sont lâches,\n",
      "Nous nous faisons payer grassement nos aveux,\n",
      "Et nous rentrons gaîment dans le chemin bourbeux,\n",
      "Croyant par de vils pleurs laver toutes nos taches.\n",
      "\n",
      "Sur l'oreiller du mal c'est Satan Trismégiste\n",
      "Qui berce longuement notre esprit enchanté,\n",
      "Et le riche métal de notre volonté\n",
      "Est tout vaporisé par ce savant chimiste.\n",
      "\n",
      "C'est le Diable qui tient les fils qui nous remuent!\n",
      "Aux objets répugnants nous trouvons des appas;\n",
      "Chaque jour vers l'Enfer nous descendons d'un pas,\n",
      "Sans horreur, à travers des ténèbres qui puent.\n",
      "\n",
      "Ainsi qu'un débauché pauvre qui baise et mange\n",
      "Le sein martyrisé d'une antique catin,\n",
      "Nous volons au passage un plaisir clandestin\n",
      "Que nous pressons bien fort comme une vieille orange.\n",
      "\n",
      "Serré, fourmillant, comme un million d'helminthes,\n",
      "Dans nos cerveaux ribote un peuple de Démons,\n",
      "Et, quand nous respirons, la Mort dans nos poumons\n",
      "Descend, fleuve invisible, avec de sourdes plaintes.\n",
      "\n",
      "Si le viol, le poison, le poignard, l'incendie,\n",
      "N'ont pas encore brodé de leurs plaisants desseins\n",
      "Le canevas banal de nos piteux destins,\n",
      "C'est que notre âme, hélas! n'est pas assez hardie.\n",
      "\n",
      "Mais parmi les chacals, les panthères, les lices,\n",
      "Les singes, les scorpions, les vautours, les serpents,\n",
      "Les monstres glapissants, hurlants, grognants, rampants\n",
      "Dans la ménagerie infâme de nos vices,\n",
      "\n",
      "Il en est un plus laid, plus méchant, plus immonde!\n",
      "Quoiqu'il ne pousse ni grands gestes ni grands cris,\n",
      "Il ferait volontiers de la terre un débris\n",
      "Et dans un bâillement avalerait le monde;\n",
      "\n",
      "C'est l'Ennui!--L'oeil chargé d'un pleur involontaire,\n",
      "Il rêve d'échafauds en fumant son houka.\n",
      "Tu le connais, lecteur, ce monstre délicat,\n",
      "--Hypocrite lecteur,--mon semblable,--mon frère!\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "SPLEEN ET IDÉAL\n",
      "\n",
      "BENEDICTION\n",
      "\n",
      "\n",
      "L\n"
     ]
    }
   ],
   "source": [
    "text_data_fname = 'baudelaire.txt'  # ~0.1m characters (French)\n",
    "# text_data_fname = 'proust.txt'      # ~7.3m characters (French)\n",
    "# text_data_fname = 'shakespeare.txt' # ~0.1m characters (English)\n",
    "# text_data_fname = 'lotr.txt'        # ~2.5m characters (English)\n",
    "# text_data_fname = 'doom.c'          # ~1m characters (C Code)\n",
    "# text_data_fname = 'linux.c'         # ~11.5m characters (C code)\n",
    "\n",
    "text_data = open(text_data_fname, 'r').read()\n",
    "text_data = text_data[:10000] # use a small subset\n",
    "print(f'Dataset `{text_data_fname}` contains {len(text_data)} characters.')\n",
    "print('Excerpt of the dataset:')\n",
    "print(text_data[:2000])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9b9a404-8b09-4ab1-9529-98ff57650ef5",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d6b030743489f86ece30d79835434297",
     "grade": false,
     "grade_id": "cell-9695c6f2cf95337c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**(Question)** Create a character-level vocabulary for your text data. Create two dictionaries: `ctoi` mapping each character to an index, and the reverse `itoc` mapping each index to its corresponding character. Implement the functions to convert text to tensor and tensor to text using these mappings. Apply these functions to some text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "885ecc8a-c654-463b-9353-f7a18a607199",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e3eb46710b4006993f4eb9c557a2376f",
     "grade": true,
     "grade_id": "vocabulary",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "unique_chars = np.unique(text_data)\n",
    "\n",
    "ctoi = {char: idx for idx, char in enumerate(unique_chars)}\n",
    "itoc = {idx: char for idx, char in enumerate(unique_chars)}\n",
    "\n",
    "# Implement the function converting text to tensor\n",
    "def text_to_tensor(text, ctoi):\n",
    "    tensor = torch.tensor([ctoi[char] for char in text if char in ctoi], dtype=torch.long)\n",
    "    return tensor\n",
    "\n",
    "# Implement the function converting tensor to text\n",
    "def tensor_to_text(tensor, itoc):\n",
    "    text = ''.join([itoc[idx.item()] for idx in tensor])\n",
    "    return text\n",
    "\n",
    "tensor = text_to_tensor(text_data, ctoi)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8da1b7f-549d-45d9-8fea-309ee0e76a90",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "fbfd5a4df6e6081581ad38d7180fddfb",
     "grade": false,
     "grade_id": "cell-172b8befc4633227",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 2. Setup a character-level recurrent neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d33cfac-93f7-4980-8510-fcf675d3a06b",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6cb30929efdd10eb6066750f4b94bf14",
     "grade": false,
     "grade_id": "embedding",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "**(Question)** Setup a simple embedding layer with `nn.Embedding` to project character indices to `embedding_dim` dimensional vectors. Explain precisely how this layer works and what are its outputs for a given input sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "609747a0-6634-4232-92cb-72946ef516b0",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a9dd84493589d73c13fac37411610a2e",
     "grade": true,
     "grade_id": "cell-a3b5adeb8111779b",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "vocab_size = len(unique_chars)\n",
    "embedding_dim = 10\n",
    "\n",
    "embedding_layer = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26f7a5e3-f137-49ef-a135-a855029b724b",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ba287efa3849a2ddcb98b94b287fd199",
     "grade": true,
     "grade_id": "cell-4065672821a5801f",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "* Functionning of the Embedding Layer:\n",
    "    * The embedding layer initializes a lookup table (a vocab_size $\\times$ embedding_dim matrix) with random or pre-trained values.\n",
    "    * During the forward pass:\n",
    "        * Each integer in the input tensor is treated as a row index.\n",
    "        * The layer retrieves the corresponding row (vector) from the lookup table.\n",
    "\n",
    "* Outputs:\n",
    "  *  For an input tensor of shape (batch_size, sequence_length), the output is a tensor of shape (batch_size, sequence_length, embedding_dim).\n",
    "  *  Each integer index in the input is replaced by its corresponding embedding vector."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ebc7b1c-6469-4c63-9965-7a7f39734302",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6897eae892e4aa50cceac9bca0a83f82",
     "grade": false,
     "grade_id": "base-rnn",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "**(Question)** Setup a single-layer RNN with `nn.RNN` (without defining a custom class). Use `hidden_dim` size for hidden states. Explain precisely the outputs of this layer for a given input sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "162dd7af-cf96-4279-b512-1507433c7826",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "095b1c904c7c5fa3d5371de208f7300a",
     "grade": true,
     "grade_id": "cell-c0b3cdd14603b6d1",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([2, 5, 10])\n",
      "Output shape: torch.Size([2, 5, 16])\n",
      "Hidden shape: torch.Size([1, 2, 16])\n"
     ]
    }
   ],
   "source": [
    "input_size = 10   # Size of embedding vectors\n",
    "hidden_dim = 16   # Size of the RNN hidden states\n",
    "batch_size = 2    # Number of sequences in a batch\n",
    "seq_length = 5    # Length of each sequence\n",
    "\n",
    "rnn = nn.RNN(input_size=input_size, hidden_size=hidden_dim, batch_first=True)\n",
    "\n",
    "input_seq = torch.randn(batch_size, seq_length, input_size)\n",
    "\n",
    "output, hidden = rnn(input_seq)\n",
    "\n",
    "print(\"Input shape:\", input_seq.shape)        # (batch_size, seq_length, input_size)\n",
    "print(\"Output shape:\", output.shape)          # (batch_size, seq_length, hidden_dim)\n",
    "print(\"Hidden shape:\", hidden.shape)          # (num_layers, batch_size, hidden_dim)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56780e84-8767-4941-ba0a-2047b106fc35",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2f3230528c7fabd5b26931934ac4e8b0",
     "grade": true,
     "grade_id": "cell-9c6c3e3359e2c37e",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76df8c7a-2a3d-43a0-9666-a797d1a3dbca",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3993b8b02d0be0e16de189c4850bbfce",
     "grade": false,
     "grade_id": "rnn-model",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "**(Question)** Create a simple RNN model with a custom `nn.Module` class. It should contain: an embedding layer, a single-layer RNN, and a dense output layer. For each character of the input sequence, the model should predict the probability of the next character. The forward method should return the probabilities for next characters and the corresponding hidden states.\n",
    "After completing the class, create a model and apply the forward pass on some input text. Understand and explain the results.\n",
    "\n",
    "*Note:* depending on how you implement the loss function later, it can be convenient to return logits instead of probabilities, i.e. raw values of the output layer before any activation function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cc835a8-a05f-4f78-b50e-4edbdb04305f",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b587c6f505f5d3719869ae8a57fcb5c6",
     "grade": true,
     "grade_id": "cell-e55d41dd89bcf74f",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class CharRNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers=1):\n",
    "        '''Initialize model parameters and layers.'''\n",
    "        super().__init__()\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def forward(self, tensor_data, hidden_state=None):\n",
    "        '''Apply the forward pass for some text data already converted to tensor.'''\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "\n",
    "# Initialize a model and apply the forward pass on some input text\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80eb09a2-2c5e-4dd6-922e-a7f0c6cbff85",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ae8f4a875df816da1463fc833e40cea4",
     "grade": true,
     "grade_id": "cell-093600fc493e6e4f",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41614741-8bf2-4ead-8205-788623404a27",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9bb6ca5b49b7c8b46253cf2a7f1200c5",
     "grade": false,
     "grade_id": "rnn-overfit",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**(Question)** Implement a simple training loop to overfit on a small input sequence. The loss function should be a categorical cross entropy on the predicted characters. Monitor the loss function value over the iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "474f97eb-685f-41c1-8249-1eb4df9e168c",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "fa15210da5aabdca4ecf3228efec88be",
     "grade": true,
     "grade_id": "cell-1904f4989149b1ef",
     "locked": false,
     "points": 3,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Sample a small input sequence into tensor `input_seq` and store its corresponding expected sequence into tensor `target_seq`\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "# Implement a training loop overfitting an input sequence and monitoring the loss function\n",
    "def train_overfit(model, input_seq, target_seq, n_iters=200, learning_rate=0.2):\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "# Initialize a model and make it overfit the input sequence\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68c66f34-7afb-4920-b6d2-5642d28a0770",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9cd6d0a90166cea96ce250f8d8f2e083",
     "grade": false,
     "grade_id": "rnn-argmax",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "**(Question)** Implement a `predict_argmax` method for your `RNN` model. Then, verify your overfitting: use some characters of your input sequence as context to predict the remaining ones. Experiment with the current model and analyze the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d0350ae-a23a-4802-a2a5-012193ede15b",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0114ed939e7dd8b656bd0f03bd576a89",
     "grade": true,
     "grade_id": "cell-2d706c010aeccb0d",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class CharRNN(CharRNN):\n",
    "    def predict_argmax(self, context_tensor, n_predictions):\n",
    "        # Apply the forward pass for the context tensor\n",
    "        # Then, store the last prediction and last hidden state\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "        # Use the last prediction and last hidden state as inputs to the next forward pass\n",
    "        # Do this in a loop to predict the next `n_predictions` characters\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "\n",
    "# Initialize a model and make it overfit as above\n",
    "# Then, verify your overfitting by predicting characters given some context\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b3e9b8b-f952-43af-a2f0-084451d85759",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "25bd5ff990e4cf55e89b26541d0acf34",
     "grade": true,
     "grade_id": "cell-b783299fd35282d3",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4f5d3e9-9a68-48b8-a549-0fc2b9f2b2ff",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f34d247477f051d257bc1337cfc611fa",
     "grade": false,
     "grade_id": "cell-52baebc1e4eb464c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Using the argmax function to predict the next character can yield a deterministic generator always predicting the same characters. Instead, it is common to predict the next character by sampling from the distribution of output predictions, adding some randomness into the generator."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5701d4df-dca5-4884-8ac2-c8efe4fe4641",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9cefa534edd726def1328ea0b48ed29d",
     "grade": false,
     "grade_id": "cell-e85a5e3954f17ad2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**(Question)** Implement a `predict_proba` method for your `RNN` model. It should be very similar to `predict_argmax`, but instead of using argmax, it should randomly sample from the output predictions. To do that, you can use the `torch.distribution.Categorical` class and its `sample()` method. Verify that your method correctly added some randomness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72da1efb-39b8-496d-9c8f-cea241c41364",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2ba3d32edc8f535eb8923fb8e71c9fe4",
     "grade": true,
     "grade_id": "rnn-sample",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class CharRNN(CharRNN):\n",
    "    def predict_proba(self, input_context, n_predictions):\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "\n",
    "# Verify that your predictions are not deterministic anymore\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9912f5cc-3627-41e8-b5ef-6d30f7c4868a",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e56bfe8d33a343e270cfe35720aeea26",
     "grade": false,
     "grade_id": "cell-6389d46b2b8abaa0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 3. Train the RNN model on text data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8024df30-af42-4ca9-be0e-16614ead56cb",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "44f87a393c4ae266b59141953d170a7e",
     "grade": false,
     "grade_id": "rnn-train",
     "locked": true,
     "points": 4,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "**(Question)** Adapt your previous code to implement a proper training loop for a text dataset. To do so, we need to specify a sequence length `seq_len`, acting similarly to the batch size in classic neural networks. Then, you can either randomly sample sequences of length `seq_len` from the text dataset over `n_iters` iterations, or properly loop over the text dataset for `n_epochs` epochs (with a random starting point for each epoch to ensure different sequences), to make sure the whole dataset is seen by the model. Feel free to adjust training and model parameters empirically. Start with a small model and a small subset of the text dataset, then move on to larger experiments. Remember to use GPU if available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f65d237e-d095-45c4-ab12-6307a5bda255",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "17faecb7751e77b7fe8cae66820687ea",
     "grade": true,
     "grade_id": "cell-a4695fabcf78e1a8",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Create the text dataset, compute its mappings and convert it to tensor\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "# Initialize training parameters\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "# Initialize a character-level RNN model\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "# Setup the training loop\n",
    "# Regularly record the loss and sample from the model to monitor what is happening\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a318abd-5c3c-462d-9944-9aec1f78446c",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "113032562a0b82e504d636abf3164360",
     "grade": false,
     "grade_id": "rnn-predict",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "**(Question)** From your trained model, play around with its predictions: start with a custom input sequence and ask the model to predict the rest. Analyze and comment your results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53d516b1-477c-4237-956a-471062dd6019",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5bd724d3a1f3ba5d0b965b9cf7905160",
     "grade": true,
     "grade_id": "cell-08bfe03b817a9908",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad02a4a5-5aab-403a-9bd3-fd752dc1dc9a",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "288f4746f3c7d1906ca99b35f4a6a6e3",
     "grade": true,
     "grade_id": "cell-6b41d47e15ea128a",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7de32122-8819-4d6b-8f7a-e96f671311d6",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2eddbf9984d6a4d51a7ea1301800bdf3",
     "grade": false,
     "grade_id": "cell-a69a65798f792cfc",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 4. Experiment with different RNN architectures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5def498-9119-45fd-8807-8260cd0a05d8",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c8895d706884667948148708d8df824e",
     "grade": false,
     "grade_id": "rnn-experiments",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "**(Question)** Experiment with different RNN architectures. Potential ideas are multi-layer RNNs, GRUs and LSTMs. All models can be extended to multi-layer using the `num_layers` parameter. Analyze and comment your results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38c6fe7d-d159-401d-9c7c-f7e9a5b86cba",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "77ebb731623feb1292e7758788ad56d4",
     "grade": true,
     "grade_id": "cell-7bbcfb8355f44b5d",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13329eaa-6a48-47cf-912d-424a79680f91",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5bffac2e6bdfed739aea204b3c40a792",
     "grade": true,
     "grade_id": "cell-3961b7f97f038a4b",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00e6f700-f9a6-4410-8e2e-98293ff2602d",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9a0fe43e34f2a57b27818be817787991",
     "grade": false,
     "grade_id": "cell-f2566f616af595f1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# (Bonus) 5. Experiment with Transformer architectures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f6cad84-9898-4f89-ba5c-a4537e4093db",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "accfb7cc955e45692a6d663280711de3",
     "grade": false,
     "grade_id": "transformers",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "**(Question)** Experiment with a small transformer based on attention layers. Try to visualize some attention maps generated by the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac2e6ff7-111b-409f-a43a-55565b17f5c1",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "96e9adc586fa33c2703e928fdabae7a7",
     "grade": true,
     "grade_id": "cell-03f69f806e85d414",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
